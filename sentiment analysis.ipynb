{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "This project uses some python 3.10 features, so make sure you have python version **>= 3.10** installed.\n",
    "\n",
    "To install the external modules used in this project run (`pip install -r requirements.txt`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After that, run then following cell to download the used nltk resources:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tom/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /home/tom/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /home/tom/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "I'm using the [SemEval-2013](https://aclanthology.org/S13-2052/) dataset for this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "\n",
    "I'm defining some functions and classes to keep the codebase clean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InvalidPatternException(Exception):\n",
    "    \"\"\"\n",
    "    Exception raised when the pattern is not supported.\n",
    "\n",
    "    Attributes:\n",
    "        pattern     The pattern that could not be matched.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, pattern: str):\n",
    "        self.pattern = pattern\n",
    "        self.message = f\"'{pattern}' is not a valid pattern\"\n",
    "\n",
    "        super().__init__(self.message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "class Sentiment(Enum):\n",
    "    \"\"\"\n",
    "    A sentiment (either POSITIVE, NEGATIVE or NEUTRAL).\n",
    "    \"\"\"\n",
    "\n",
    "    POSITIVE = 1,\n",
    "    NEGATIVE = 2,\n",
    "    NEUTRAL = 0,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sentiment(s: str) -> Sentiment:\n",
    "    \"\"\"\n",
    "    Matches a sentiment in form of a string from the dataset to a Sentiment.\n",
    "\n",
    "    :param s: The 'sentiment' string from the dataset.\n",
    "\n",
    "    :raises InvalidPatternException: If s does not matches any value.\n",
    "    \"\"\"\n",
    "\n",
    "    match s:\n",
    "        case \"positive\": return Sentiment.POSITIVE\n",
    "        case \"negative\": return Sentiment.NEGATIVE\n",
    "        case \"neutral\": return Sentiment.NEUTRAL\n",
    "        case _: raise InvalidPatternException(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class DataPoint():\n",
    "    \"\"\"\n",
    "    A dataclass that represents a 'unit' of data.\n",
    "\n",
    "    Attributes:\n",
    "        sentiment   The sentiment of the tweet.\n",
    "        tweet       A list of words containing the tweet.\n",
    "    \"\"\"\n",
    "\n",
    "    sentiment: Sentiment\n",
    "    tweet: list[str]\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "\n",
    "def read_data(file: str, lang: str = \"english\",\n",
    "              remove_unicode: bool = True,\n",
    "              remove_links: bool = True,\n",
    "              do_remove_punctuation: bool = True) -> list[DataPoint]:\n",
    "    \"\"\"\n",
    "    Reads the raw data from a file and creates a list of DataPoint objects.\n",
    "\n",
    "    :param file:    The file to read the data from.\n",
    "    :returns:       List of DataPoint objects.\n",
    "\n",
    "    :raises AssertionError: If a line does not match the regular expression. This should never happen.\n",
    "    \"\"\"\n",
    "\n",
    "    data = list()\n",
    "    with open(file) as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    pattern = r\"^([0-9]+)\\s+(positive|negative|neutral)\\s+\\\"?(.+)\\\"?$\"\n",
    "    for line in lines:\n",
    "        match = re.match(pattern, line)\n",
    "\n",
    "        assert match\n",
    "        \n",
    "        clean_string = match.group(3)\n",
    "        \n",
    "        if remove_links:\n",
    "            clean_string = re.sub(r\"https?:\\/\\/www\\.[a-z0-9]+\\.[a-z]+(\\.[a-z]+)?(\\/[a-z0-9]+)*\", \"\", clean_string)\n",
    "            \n",
    "        if remove_unicode:\n",
    "            clean_string = re.sub(r\"\\\\u[a-fA-F0-9]+\", \"\", clean_string)\n",
    "        \n",
    "        if do_remove_punctuation:\n",
    "            clean_string = re.sub(r\"[#@\\.;,\\?!:\\-%\\\\'\\$]\", \"\", clean_string)\n",
    "\n",
    "        sentiment = create_sentiment(match.group(2))\n",
    "        words = word_tokenize(clean_string, lang)\n",
    "\n",
    "        data_point = DataPoint(sentiment, words)\n",
    "\n",
    "        data.append(data_point)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pre-processing the data\n",
    "\n",
    "Apart 'cleaning' the data (removing links, unicode and punctuation), I'm also doing some prepocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use the `stopwords` from the `nltk.corpus` package to remove stopwords from the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stopwords(s:list[str], lang: str = \"english\") -> list[str]:\n",
    "    \"\"\"\n",
    "    Removes stopwords from a given list of words.\n",
    "\n",
    "    :param s: The input list of words.\n",
    "    :returns: The input list without stopwords.\n",
    "    \"\"\"\n",
    "\n",
    "    words = [word for word in s if word not in stopwords.words(lang)]\n",
    "\n",
    "    return words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemmers and/or Lemmatizers can be used to 'normalize' the words. Additionally the words can be converted to all lower case letters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Optional\n",
    "\n",
    "def normalize(s: list[str],\n",
    "              lower: bool = False,\n",
    "              Stemmer = None,\n",
    "              stemmer_args: Optional[list] = None,\n",
    "              Lemmatizer = None) -> list[str]:\n",
    "    \"\"\"\n",
    "    Normalizes a list of words using methods specified in the function parameters.\n",
    "\n",
    "\n",
    "    :param Stemmer:      A Stemmer that can optionally be used in normalization.\n",
    "    :param stemmer_args: The constructor arguments of the Stemmer (if any).\n",
    "\n",
    "    :param Lemmatizer: A Lemmatizer that can optionally be used in normalization.\n",
    "\n",
    "    :returns: The normalized list of words.\n",
    "    \"\"\"\n",
    "    \n",
    "    if lower:\n",
    "        s = [word.lower() for word in s]\n",
    "\n",
    "    if Stemmer:\n",
    "        stemmer = Stemmer(*stemmer_args) if stemmer_args else Stemmer()\n",
    "\n",
    "        s = [stemmer.stem(word) for word in s]\n",
    "\n",
    "    if Lemmatizer:\n",
    "        lemmatizer = Lemmatizer()\n",
    "\n",
    "        s = [lemmatizer.lemmatize(word) for word in s]\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'm defining a wrapper function that I can configure so that these functions don't have to be called by themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(data: list[DataPoint],\n",
    "                    do_remove_stopwords: bool = False,\n",
    "                    do_normalize: bool = False,\n",
    "                    lower: bool = False,\n",
    "                    stemmer = None,\n",
    "                    stemmer_args = None,\n",
    "                    lemmatizer = None) -> list[DataPoint]:\n",
    "    \"\"\"\n",
    "    Preprocesses the data.\n",
    "\n",
    "    :param data:               A list of data points containing the data.\n",
    "    :param remove_stopwords:   Tells the function to remove the stopwords if true.\n",
    "    :param do_normalize:       Tells the function to do some normalization if true.\n",
    "    :param lower:              Converty every letter to lower case if true.\n",
    "    :param stemmer:            The stemmer that is supposed to be used. Only takes effect if 'do_normalize' is True.\n",
    "    :param stemmer_args:       Arguements for the stemmer. Only takes effect if 'do_normalize' is True and a Stemmer has been selected.\n",
    "    :param lemmatizer:         The lemmatizer that is supposed to be used. Only takes effect if 'do_normalize' is True.\n",
    "\n",
    "    :returns: A processed list of data points.\n",
    "    \"\"\"\n",
    "\n",
    "    for d in data:\n",
    "        tweet = d.tweet\n",
    "\n",
    "        if do_remove_stopwords:\n",
    "            tweet = remove_stopwords(tweet)\n",
    "\n",
    "        if do_normalize:\n",
    "            \n",
    "            tweet = normalize(tweet,\n",
    "                              lower=lower,\n",
    "                              Stemmer=stemmer,\n",
    "                              stemmer_args=stemmer_args,\n",
    "                              Lemmatizer=lemmatizer)\n",
    "\n",
    "        d.tweet = tweet\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the features and creating a \"bag of words\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(train_data: list[DataPoint]) -> list[str]:\n",
    "  \"\"\"\n",
    "  Extracts the features from the training data.\n",
    "  \n",
    "  This is done by creating a list that contains every word in the dataset.\n",
    "  So every word basically is a feature.\n",
    "  \n",
    "  :param train_data: is the training data that the feature set is built upon.\n",
    "\n",
    "  :returns: A list that contains each word in the dataset once.\n",
    "  \"\"\"\n",
    "\n",
    "  wordlist = set()\n",
    "\n",
    "  for d in train_data:\n",
    "      wordlist.update(d.tweet)\n",
    "\n",
    "  wordlist = list(wordlist)\n",
    "\n",
    "  return wordlist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def create_bag_of_words(\n",
    "        wordlist: list[str],\n",
    "        data: list[DataPoint],\n",
    "        force_positive_numbers: bool = False) -> list[list[int]]:\n",
    "    \"\"\"\n",
    "    Creates an NxM matrix where N is the total number of unique words in the dataset\n",
    "    and M is the amount of tweets.\n",
    "\n",
    "    The matrix is filled with integers, where the integer for each word represents how\n",
    "    often it appears in that tweet.\n",
    "\n",
    "    So if we had three tweets looking like this:\n",
    "\n",
    "    I did good.\n",
    "    I went to the concert.\n",
    "    We went to see him.\n",
    "\n",
    "    And assuming the wordlist generated would look like this:\n",
    "\n",
    "    [\"I\", \"did\", \"good\", \"went\", \"to\", \"the\", \"concert\", \"We\", \"see\", \"him\"]\n",
    "\n",
    "    The matrix would look like this:\n",
    "\n",
    "    [\n",
    "    [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
    "    [1, 0, 0, 1, 1, 1, 1, 0, 0, 0]\n",
    "    [0, 0, 0, 1, 1, 0, 0, 1, 1, 1]\n",
    "    ]\n",
    "\n",
    "    Negative number are possible:\n",
    "\n",
    "    I did not good.\n",
    "\n",
    "    Generates the following matrix:\n",
    "\n",
    "    [[1, 1, 1, -1]]\n",
    "\n",
    "    The reason for this is that everything after a \"not\" is inverted.\n",
    "    This can be avoided by using the 'force_positive_numbers' flag and setting it to true.\n",
    "\n",
    "    :param wordlist:                the list of words that are defined as features.\n",
    "    :param data:                    the list of data points that represents the data set.\n",
    "    :param force_positive_numbers:  optional flag to avoid negative values in the matrix.\n",
    "    \"\"\"\n",
    "\n",
    "    bag_of_words = list()\n",
    "\n",
    "    for d in data:\n",
    "      mod = 1\n",
    "      word_vector = np.zeros((len(wordlist), ), dtype=np.int64)\n",
    "      for word in d.tweet:\n",
    "          try:\n",
    "              word_index = wordlist.index(word)\n",
    "              word_vector[word_index] += (1 * mod)\n",
    "              if word == \"not\":\n",
    "                  mod *= -1\n",
    "          except ValueError:\n",
    "              # NOTE: We just skip the word if it's not in our wordlist\n",
    "              continue\n",
    "\n",
    "      bag_of_words.append(word_vector)\n",
    "\n",
    "    if force_positive_numbers:\n",
    "        for lst in bag_of_words:\n",
    "            for i, value in enumerate(lst):\n",
    "                lst[i] = max(value, 0)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return bag_of_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the data\n",
    "\n",
    "Now I can start reading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = read_data(\"./semeval2013/twitter-2013train-A.txt\")\n",
    "dev_data = read_data(\"./semeval2013/twitter-2013dev-A.txt\", do_remove_punctuation=False)\n",
    "test_data = read_data(\"./semeval2013/twitter-2013test-A.txt\", do_remove_punctuation=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing and extracting features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess_data(train_data, do_remove_stopwords=True, do_normalize=False)\n",
    "\n",
    "word_list = extract_features(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the training and testing sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = create_bag_of_words(word_list, data)\n",
    "y_train = [d.sentiment._value_[0] for d in data]\n",
    "\n",
    "x_dev = create_bag_of_words(word_list, dev_data)\n",
    "y_dev = [d.sentiment._value_[0] for d in dev_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier\n",
    "\n",
    "I'm using the [Random Forest Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) for this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:   14.5s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:   44.1s\n",
      "[Parallel(n_jobs=50)]: Done 700 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=50)]: Done 900 out of 900 | elapsed:  1.8min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(class_weight='balanced', max_depth=8, n_estimators=900,\n",
       "                       n_jobs=50, verbose=1)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "random_forest_classifier = RandomForestClassifier(n_estimators=900,\n",
    "                                                  max_depth=8,\n",
    "                                                  verbose=1,\n",
    "                                                  n_jobs=50,\n",
    "                                                  class_weight=\"balanced\")\n",
    "\n",
    "\n",
    "random_forest_classifier.fit(x_train, y_train)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.3s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.73      0.79      5381\n",
      "           1       0.65      0.83      0.73      2826\n",
      "           2       0.68      0.67      0.67      1477\n",
      "\n",
      "    accuracy                           0.75      9684\n",
      "   macro avg       0.73      0.74      0.73      9684\n",
      "weighted avg       0.77      0.75      0.75      9684\n",
      "\n",
      "[[3930 1081  370]\n",
      " [ 370 2354  102]\n",
      " [ 286  205  986]]\n",
      "0.7507228418009088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 700 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=50)]: Done 900 out of 900 | elapsed:    0.6s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "\n",
    "predictions = random_forest_classifier.predict(x_train)\n",
    "\n",
    "print(classification_report(predictions, y_train))\n",
    "print(confusion_matrix(predictions, y_train))\n",
    "print(accuracy_score(predictions, y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.1s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.61      0.70       987\n",
      "           1       0.49      0.70      0.58       399\n",
      "           2       0.46      0.59      0.52       268\n",
      "\n",
      "    accuracy                           0.63      1654\n",
      "   macro avg       0.59      0.63      0.60      1654\n",
      "weighted avg       0.68      0.63      0.64      1654\n",
      "\n",
      "[[601 250 136]\n",
      " [ 72 281  46]\n",
      " [ 66  44 158]]\n",
      "0.6287787182587666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 700 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=50)]: Done 900 out of 900 | elapsed:    0.3s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "predictions = random_forest_classifier.predict(x_dev)\n",
    "\n",
    "print(classification_report(predictions, y_dev))\n",
    "print(confusion_matrix(predictions, y_dev))\n",
    "print(accuracy_score(predictions, y_dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Using backend ThreadingBackend with 50 concurrent workers.\n",
      "[Parallel(n_jobs=50)]: Done 100 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=50)]: Done 350 tasks      | elapsed:    0.2s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.55      0.69      2505\n",
      "           1       0.42      0.84      0.56       736\n",
      "           2       0.32      0.58      0.41       306\n",
      "\n",
      "    accuracy                           0.61      3547\n",
      "   macro avg       0.55      0.66      0.55      3547\n",
      "weighted avg       0.76      0.61      0.64      3547\n",
      "\n",
      "[[1382  784  339]\n",
      " [  76  618   42]\n",
      " [  55   73  178]]\n",
      "0.6140400338314068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=50)]: Done 700 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=50)]: Done 900 out of 900 | elapsed:    0.4s finished\n"
     ]
    }
   ],
   "source": [
    "x_test = create_bag_of_words(word_list, test_data)\n",
    "y_test = [d.sentiment._value_[0] for d in test_data]\n",
    "\n",
    "\n",
    "predictions = random_forest_classifier.predict(x_test)\n",
    "\n",
    "print(classification_report(predictions, y_test))\n",
    "print(confusion_matrix(predictions, y_test))\n",
    "print(accuracy_score(predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "In this model I can reach an accuracy of **62.73%** on the validation set.\n",
    "The f1-scores are also decent accross the board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "384px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
